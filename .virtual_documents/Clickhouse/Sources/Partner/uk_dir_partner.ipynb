


import clickhouse_connect
import datetime
import os
import pytz
import pandas as pd
from dateutil.relativedelta import relativedelta
from dotenv import load_dotenv

import sys
sys.path.append('/home/boris/Documents/Work/analytics/Clickhouse')
from clickhouse_client import ClickHouse_client
ch = ClickHouse_client()
pd.set_option('display.max_rows', 1000)







# creating a table from s3

query_text = """--sql
    CREATE TABLE db1.uk_dir_partner
    (
        `id` Int32,
        `name`  String,
        `partner_uuid_uk` String,
        `created_at` String
    )
    ENGINE = S3('https://storage.yandexcloud.net/dwh-asgard/uk_dir_partner/uk_dir_partner.csv','CSVWithNames')
    """

ch.query_run(query_text)


# creating a table for materialized view

query_text = """--sql
    CREATE TABLE db1.uk_dir_partner_ch
    (
        `id` Int32,
        `name`  String,
        `partner_uuid_uk` String,
        `created_at` String
    )
    ENGINE = MergeTree()
    ORDER BY partner_uuid_uk
    """

ch.query_run(query_text)


# creating a materialized view

query_text = """--sql
    CREATE MATERIALIZED VIEW db1.uk_dir_partner_mv
    REFRESH EVERY 1 DAY OFFSET 3 HOUR RANDOMIZE FOR 1 HOUR TO db1.uk_dir_partner_ch AS
    SELECT
        *
    FROM db1.uk_dir_partner
    """

ch.query_run(query_text)





query_text = """--sql
    SELECT
        *
    FROM db1.uk_dir_partner_ch
    limit 10
    """

ch.query_run(query_text)





query_text = """
    DROP TABLE db1.uk_dir_partner_ch
    """
ch.query_run(query_text)





query_text = """
    DROP TABLE db1.uk_dir_partner_mv
    """
ch.query_run(query_text)
