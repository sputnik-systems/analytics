{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import boto3\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ACCESS_KEY,SECRET_KEY,TOKEN,FOLDER,FOLDER_ID,BUCKET_NAME,BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_ZONE = os.getenv(\"TIME_ZONE\", \"Europe/Moscow\") #настройка функции\n",
    "TEMP_FILENAME = \"temp_file\"\n",
    "\n",
    "headers={'Authorization':TOKEN ,'Accept':'application/json'}\n",
    "\n",
    "def get_now_datetime_str(): # получаем актуальное время\n",
    "    time_zone = os.getenv(\"TIME_ZONE\", \"Europe/Moscow\") # меняем таймзону на московскую\n",
    "    now = datetime.datetime.now(pytz.timezone(time_zone))    \n",
    "    yesterday = now - datetime.timedelta(days=1) #нужна вчерашняя дата так как данные за прошлый день\n",
    "    last_month_data = now - relativedelta(month=1)\n",
    "    return {'key_parquet': yesterday.strftime('year=%Y/month=%m/%d.parquet'),\n",
    "            'key': yesterday.strftime('year=%Y/month=%m/%d.csv'),\n",
    "            'key_month': yesterday.strftime('year=%Y/month=%m.csv'),\n",
    "            'now':now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'yesterday_data':yesterday.strftime('%Y-%m-%d'),\n",
    "            'yesterday':yesterday.strftime('%Y-%m-%d %H:%M:%S'), \n",
    "            'year':yesterday.strftime('%Y'),\n",
    "            'month':yesterday.strftime('%m'),\n",
    "            'day':yesterday.strftime('%d'),\n",
    "            'last_month_data':last_month_data.strftime('%Y-%m-%d')\n",
    "            }\n",
    "\n",
    "def create_query(): #функция создает новый запрос и возвращает id для запроса результата\n",
    "    body = {\n",
    "        \"name\":query_name, \n",
    "        \"TYPE\":\"ANALYTICS\", \n",
    "        \"text\":query_text, \n",
    "        \"description\":query_description\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f'https://api.yandex-query.cloud.yandex.net/api/fq/v1/queries?project={FOLDER_ID}',\n",
    "        headers=headers,\n",
    "        json=body\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"id\"]\n",
    "    return f' Code: {response},  text: {response.text}'\n",
    "\n",
    "\n",
    "def get_request(offset): # фунция возвращает ответ запроса. Максимум 1000 строк.\n",
    "    offset = offset\n",
    "    get_query_results_url = f'https://api.yandex-query.cloud.yandex.net/api/fq/v1/queries/{request_id}/results/0?project={FOLDER_ID}&offset={str(offset)}&limit=1000'\n",
    "    response = requests.get(\n",
    "        get_query_results_url,\n",
    "        headers = headers\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def if_cell_is_list(cell): # функция участвует в преобразовании данных при создании файла\n",
    "    if isinstance(cell, list):\n",
    "        if len(cell) == 0:\n",
    "            return ''\n",
    "        else: \n",
    "            return cell[0]\n",
    "    else:\n",
    "        return cell\n",
    "\n",
    "def write_temp_file():\n",
    "    offset = 0\n",
    "    response = get_request(offset) #запрашиваем данные запроса\n",
    "    columns = [rows['name'] for rows in response.json()['columns']] #выделяем названия столбцов\n",
    "    special_str = \"\"\n",
    "    for j in columns:\n",
    "        special_str = f\"{special_str}{str(j)},\"\n",
    "    temp_file = open(TEMP_FILENAME, 'w')\n",
    "    temp_file.write(special_str[:-1]+'\\n')\n",
    "\n",
    "def write_temp_file():\n",
    "    offset = 0\n",
    "    response = get_request(offset) #запрашиваем данные запроса\n",
    "    columns = [rows['name'] for rows in response.json()['columns']] #выделяем названия столбцов\n",
    "    special_str = \"\"\n",
    "    for j in columns:\n",
    "        special_str = f\"{special_str}{str(j)},\"\n",
    "    temp_file = open(TEMP_FILENAME, 'w', encoding='utf-8')\n",
    "    temp_file.write(special_str[:-1]+'\\n')\n",
    "\n",
    "    while response.status_code == 200 and len(response.json()['rows']) != 0:  #Цикл делает запросы по 10000, пока не кончатся данные\n",
    "        response = get_request(offset)\n",
    "        response_rows = response.json()['rows']\n",
    "        rows = [[if_cell_is_list(cell) for cell in row] for row in response_rows]  #Преобразуются строки\n",
    "        # Открывает созданный файл и добавляет в него строки\n",
    "        for row in rows:\n",
    "            special_str = ','.join(\"'{0}'\".format(i.replace(\"'\", \"\"))  if isinstance(i, str) else str(i) for i in row)\n",
    "            temp_file.write(special_str+'\\n') \n",
    "        offset +=1000 # увеличивает смещение\n",
    "\n",
    "def get_s3_instance(): # функция создает соединение\n",
    "    session = boto3.session.Session()\n",
    "    return session.client(\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "\n",
    "def upload_dump_to_s3(): # функция выгружает данные в s3\n",
    "    get_s3_instance().upload_file(\n",
    "        Filename=TEMP_FILENAME,\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=key\n",
    "    )\n",
    "\n",
    "def remove_temp_files(): #функция удаляет временный файл\n",
    "    os.remove(TEMP_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = get_s3_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flats_st_partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dwh-asgard\"\n",
    "FOLDER = 'flats_st_partner'\n",
    "s3_file_name = f\"{BUCKET_NAME}/{FOLDER}/{get_now_datetime_str()['key']}\"\n",
    "s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "path_on_pc = 'E:/s3'\n",
    "local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "if os.path.isfile(local_download_path) is False:\n",
    "# создается новая папка, если ее нет\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "    s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats_st_partner_df = pd.read_csv('local_download_path',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dwh-asgard\"\n",
    "FOLDER = 'flats_dir_partner'\n",
    "s3_file_name = f\"{BUCKET_NAME}/{FOLDER}/flats_dir_partner.csv\"\n",
    "s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "path_on_pc = 'E:/s3'\n",
    "local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "if os.path.isfile(local_download_path) is False:\n",
    "# создается новая папка, если ее нет\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "    s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats_dir_partner_df = pd.read_csv('local_download_path',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats_st_partner_df = flats_st_partner_df.merge(\n",
    "    flats_dir_partner_df['flat_uuid','address_uuid'],\n",
    "    on='flat_uuid'\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installation_point_st_partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dwh-asgard\"\n",
    "FOLDER = 'installation_point_st_partner'\n",
    "s3_file_name = f\"{BUCKET_NAME}/{FOLDER}/{get_now_datetime_str()['key']}\"\n",
    "s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "path_on_pc = 'E:/s3'\n",
    "local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "if os.path.isfile(local_download_path) is False:\n",
    "# создается новая папка, если ее нет\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "    s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_point_st_partner_df = pd.read_csv('local_download_path',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dwh-asgard\"\n",
    "FOLDER = 'entries_installation_points_dir_partner'\n",
    "s3_file_name = f\"{BUCKET_NAME}/{FOLDER}/entries_installation_points_dir_partner.csv\"\n",
    "s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "path_on_pc = 'E:/s3'\n",
    "local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "if os.path.isfile(local_download_path) is False:\n",
    "# создается новая папка, если ее нет\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "    s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_installation_points_dir_partner_df = pd.read_csv('local_download_path',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_point_st_partner_df = installation_point_st_partner_df.merge(\n",
    "    entries_installation_points_dir_partner_df['installation_point_id','address_uuid','city','сountry','region','parent_uuid'],\n",
    "    on='installation_point_id'\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entries_st_mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dwh-asgard\"\n",
    "FOLDER = 'entries_st_mobile'\n",
    "s3_file_name = f\"{BUCKET_NAME}/{FOLDER}/{get_now_datetime_str()['key']}\"\n",
    "s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "path_on_pc = 'E:/s3'\n",
    "local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "if os.path.isfile(local_download_path) is False:\n",
    "# создается новая папка, если ее нет\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "    s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_st_mobile_df = pd.read_csv('local_download_path',sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats_st_partner_df_merged = flats_st_partner_df.merge(\n",
    "    installation_point_st_partner_df['installation_point_id','address_uuid','city','сountry','region','parent_uuid'],\n",
    "    on='address_uuid',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "flats_st_partner_df_merged=flats_st_partner_df_merged.merge(\n",
    "    entries_st_mobile_df['address_uuid','monetization']\n",
    "    on='address_uuid',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats_st_partner_df_merged.to_parquet(f'{path_on_pc}/{FOLDER}/{get_now_datetime_str()['key_parquet']}', compression='snappy', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"aggregated-data\"\n",
    "FOLDER = 'flats_research_dashboard'\n",
    "\n",
    "s3_file_path = f\"{FOLDER}/{get_now_datetime_str()['key_parquet']}\"\n",
    "pc_file_path = f\"{path_on_pc}/{FOLDER}/{get_now_datetime_str()['key_parquet']}\"\n",
    "\n",
    "s3.upload_file(pc_file_path, BUCKET_NAME, s3_file_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
