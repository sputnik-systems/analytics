{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import boto3\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ACCESS_KEY,SECRET_KEY,TOKEN,FOLDER,FOLDER_ID,BUCKET_NAME,BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_ZONE = os.getenv(\"TIME_ZONE\", \"Europe/Moscow\") #настройка функции\n",
    "TEMP_FILENAME = \"temp_file\"\n",
    "\n",
    "headers={'Authorization':TOKEN ,'Accept':'application/json'}\n",
    "\n",
    "def get_now_datetime_str(): # получаем актуальное время\n",
    "    time_zone = os.getenv(\"TIME_ZONE\", \"Europe/Moscow\") # меняем таймзону на московскую\n",
    "    now = datetime.datetime.now(pytz.timezone(time_zone))    \n",
    "    yesterday = now - datetime.timedelta(days=1) #нужна вчерашняя дата так как данные за прошлый день\n",
    "    last_month_data = now - relativedelta(month=1)\n",
    "    return {'key_parquet': yesterday.strftime('year=%Y/month=%m/%d.parquet'),\n",
    "            'key': yesterday.strftime('year=%Y/month=%m/%d.csv'),\n",
    "            'key_month': yesterday.strftime('year=%Y/month=%m.csv'),\n",
    "            'now':now.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'yesterday_data':yesterday.strftime('%Y-%m-%d'),\n",
    "            'yesterday':yesterday.strftime('%Y-%m-%d %H:%M:%S'), \n",
    "            'year':yesterday.strftime('%Y'),\n",
    "            'month':yesterday.strftime('%m'),\n",
    "            'day':yesterday.strftime('%d'),\n",
    "            'last_month_data':last_month_data.strftime('%Y-%m-%d')\n",
    "            }\n",
    "\n",
    "def create_query(): #функция создает новый запрос и возвращает id для запроса результата\n",
    "    body = {\n",
    "        \"name\":query_name, \n",
    "        \"TYPE\":\"ANALYTICS\", \n",
    "        \"text\":query_text, \n",
    "        \"description\":query_description\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f'https://api.yandex-query.cloud.yandex.net/api/fq/v1/queries?project={FOLDER_ID}',\n",
    "        headers=headers,\n",
    "        json=body\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"id\"]\n",
    "    return f' Code: {response},  text: {response.text}'\n",
    "\n",
    "\n",
    "def get_request(offset): # фунция возвращает ответ запроса. Максимум 1000 строк.\n",
    "    offset = offset\n",
    "    get_query_results_url = f'https://api.yandex-query.cloud.yandex.net/api/fq/v1/queries/{request_id}/results/0?project={FOLDER_ID}&offset={str(offset)}&limit=1000'\n",
    "    response = requests.get(\n",
    "        get_query_results_url,\n",
    "        headers = headers\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def if_cell_is_list(cell): # функция участвует в преобразовании данных при создании файла\n",
    "    if isinstance(cell, list):\n",
    "        if len(cell) == 0:\n",
    "            return ''\n",
    "        else: \n",
    "            return cell[0]\n",
    "    else:\n",
    "        return cell\n",
    "\n",
    "def write_temp_file():\n",
    "    offset = 0\n",
    "    response = get_request(offset) #запрашиваем данные запроса\n",
    "    columns = [rows['name'] for rows in response.json()['columns']] #выделяем названия столбцов\n",
    "    special_str = \"\"\n",
    "    for j in columns:\n",
    "        special_str = f\"{special_str}{str(j)},\"\n",
    "    temp_file = open(TEMP_FILENAME, 'w')\n",
    "    temp_file.write(special_str[:-1]+'\\n')\n",
    "\n",
    "def write_temp_file():\n",
    "    offset = 0\n",
    "    response = get_request(offset) #запрашиваем данные запроса\n",
    "    columns = [rows['name'] for rows in response.json()['columns']] #выделяем названия столбцов\n",
    "    special_str = \"\"\n",
    "    for j in columns:\n",
    "        special_str = f\"{special_str}{str(j)},\"\n",
    "    temp_file = open(TEMP_FILENAME, 'w', encoding='utf-8')\n",
    "    temp_file.write(special_str[:-1]+'\\n')\n",
    "\n",
    "    while response.status_code == 200 and len(response.json()['rows']) != 0:  #Цикл делает запросы по 10000, пока не кончатся данные\n",
    "        response = get_request(offset)\n",
    "        response_rows = response.json()['rows']\n",
    "        rows = [[if_cell_is_list(cell) for cell in row] for row in response_rows]  #Преобразуются строки\n",
    "        # Открывает созданный файл и добавляет в него строки\n",
    "        for row in rows:\n",
    "            special_str = ','.join(\"'{0}'\".format(i.replace(\"'\", \"\"))  if isinstance(i, str) else str(i) for i in row)\n",
    "            temp_file.write(special_str+'\\n') \n",
    "        offset +=1000 # увеличивает смещение\n",
    "\n",
    "def get_s3_instance(): # функция создает соединение\n",
    "    session = boto3.session.Session()\n",
    "    return session.client(\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "\n",
    "def upload_dump_to_s3(): # функция выгружает данные в s3\n",
    "    get_s3_instance().upload_file(\n",
    "        Filename=TEMP_FILENAME,\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=key\n",
    "    )\n",
    "\n",
    "def remove_temp_files(): #функция удаляет временный файл\n",
    "    os.remove(TEMP_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = get_s3_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.strptime('2024-01-01','%Y-%m-%d').date()\n",
    "end_date = datetime.datetime.strptime('2024-05-07','%Y-%m-%d').date()\n",
    "dates_pd = pd.DataFrame({\n",
    "        'date_range': pd.date_range(start=start_date, end=end_date),\n",
    "        'date_key': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m/%d.csv'),\n",
    "        'date_key_parquet': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m/%d.parquet'),\n",
    "        'year': pd.date_range(start=start_date, end=end_date).strftime('%Y'),\n",
    "        'month': pd.date_range(start=start_date, end=end_date).strftime('%m'),\n",
    "        'day': pd.date_range(start=start_date, end=end_date).strftime('%d'),\n",
    "        'date_key_folder': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m')\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    year = dates_pd.loc[0,['year']].values[0]\n",
    "    month = dates_pd.loc[0,['month']].values[0]\n",
    "    day = dates_pd.loc[0,['day']].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Подгрузка companies_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/{FOLDER}.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка companies_st_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_st_partner'\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем companies_st_partner и companies_dir_partner\n",
    "\n",
    "    companies_st_partner_df = companies_st_partner_df.merge(\n",
    "        companies_dir_partner_df,\n",
    "        on='partner_uuid',\n",
    "        how='left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6615 entries, 0 to 6614\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   report_date        6615 non-null   object\n",
      " 1   partner_uuid       6615 non-null   object\n",
      " 2   is_blocked         6615 non-null   int64 \n",
      " 3   pro_subs           6615 non-null   int64 \n",
      " 4   enterprise_subs    6615 non-null   int64 \n",
      " 5   billing_pro        6615 non-null   int64 \n",
      " 6   balance            6615 non-null   int64 \n",
      " 7   company_name       6587 non-null   object\n",
      " 8   partner_lk         6615 non-null   int64 \n",
      " 9   registration_date  6615 non-null   object\n",
      " 10  tin                654 non-null    object\n",
      " 11  kpp                376 non-null    object\n",
      "dtypes: int64(6), object(6)\n",
      "memory usage: 620.3+ KB\n"
     ]
    }
   ],
   "source": [
    "companies_st_partner_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,dates_pd.shape[0]):\n",
    "    year = dates_pd.loc[i,['year']].values[0]\n",
    "    month = dates_pd.loc[i,['month']].values[0]\n",
    "    day = dates_pd.loc[i,['day']].values[0]\n",
    "    \n",
    "    # Подгрузка flats_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'flats_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/flats_dir_partner.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    flats_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка flats_st_partner\n",
    "\n",
    "\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'flats_st_partner'\n",
    "\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    flats_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем flats_st_partner и flats_dir_partner\n",
    "\n",
    "    flats_st_partner_df = flats_st_partner_df.merge(\n",
    "        flats_dir_partner_df[['flat_uuid','address_uuid']],\n",
    "        on='flat_uuid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Подгрузка entries_installation_points_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'entries_installation_points_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/entries_installation_points_dir_partner.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    entries_installation_points_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка installation_point_st_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'installation_point_st_partner'\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    installation_point_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем entries_installation_points_dir_partner и installation_point_st_partner\n",
    "\n",
    "    installation_point_st_partner_df = installation_point_st_partner_df.merge(\n",
    "        entries_installation_points_dir_partner_df[['installation_point_id','address_uuid','city','country','region','parent_uuid']],\n",
    "        on='installation_point_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Подгрузка companies_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/{FOLDER}.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка companies_st_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_st_partner'\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем companies_st_partner и companies_dir_partner\n",
    "\n",
    "    companies_st_partner_df = companies_st_partner_df.merge(\n",
    "        companies_dir_partner_df,\n",
    "        on='partner_uuid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Объеденяем все данные\n",
    "\n",
    "    flats_st_partner_df_merged = flats_st_partner_df.merge(\n",
    "        installation_point_st_partner_df[['digital_keys_count','device_keys_count','monetization','partner_uuid','monetization_is_allowed','installation_point_id','address_uuid','city','country','region','parent_uuid']],\n",
    "        on='address_uuid',\n",
    "        how='left'\n",
    "        )\n",
    "\n",
    "    columns = ['pro_subs','is_blocked','pro_subs','enterprise_subs','billing_pro','balance','company_name','partner_lk','registration_date','tin','kpp','partner_uuid']\n",
    "    flats_st_partner_df_merged = flats_st_partner_df_merged.merge(\n",
    "        companies_st_partner_df[columns],\n",
    "        on='partner_uuid',\n",
    "        how='left'\n",
    "        )\n",
    "\n",
    "    BUCKET_NAME = \"aggregated-data\"\n",
    "    FOLDER = 'flats_research_dashboard'\n",
    "\n",
    "    local_download_folder= f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[i,['date_key_folder']].values[0]}\"\n",
    "    local_download_path = f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[i,['date_key_parquet']].values[0]}\"\n",
    "\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "\n",
    "    flats_st_partner_df_merged.to_parquet(local_download_path, compression='snappy', index=False)\n",
    "\n",
    "    s3_file_path = f\"{FOLDER}/{dates_pd.loc[i,['date_key_parquet']].values[0]}\"\n",
    "    pc_file_path = f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[i,['date_key_parquet']].values[0]}\"\n",
    "\n",
    "    s3.upload_file(pc_file_path, BUCKET_NAME, s3_file_path)\n",
    "    print(f'{s3_file_path} the file has already been uploaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    year = dates_pd.loc[0,['year']].values[0]\n",
    "    month = dates_pd.loc[0,['month']].values[0]\n",
    "    day = dates_pd.loc[0,['day']].values[0]\n",
    "    \n",
    "    # Подгрузка flats_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'flats_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/flats_dir_partner.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    flats_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка flats_st_partner\n",
    "\n",
    "\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'flats_st_partner'\n",
    "\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    flats_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем flats_st_partner и flats_dir_partner\n",
    "\n",
    "    flats_st_partner_df = flats_st_partner_df.merge(\n",
    "        flats_dir_partner_df[['flat_uuid','address_uuid']],\n",
    "        on='flat_uuid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Подгрузка entries_installation_points_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'entries_installation_points_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/entries_installation_points_dir_partner.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    entries_installation_points_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка installation_point_st_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'installation_point_st_partner'\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    installation_point_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем entries_installation_points_dir_partner и installation_point_st_partner\n",
    "\n",
    "    installation_point_st_partner_df = installation_point_st_partner_df.merge(\n",
    "        entries_installation_points_dir_partner_df[['installation_point_id','address_uuid','city','country','region','parent_uuid']],\n",
    "        on='installation_point_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Подгрузка companies_dir_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_dir_partner'\n",
    "    s3_file_name = f\"{FOLDER}/{FOLDER}.csv\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:1])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_dir_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Подгрузка companies_st_partner\n",
    "\n",
    "    BUCKET_NAME = \"dwh-asgard\"\n",
    "    FOLDER = 'companies_st_partner'\n",
    "    key = f\"year={int(year)}/month={int(month)}/{int(day)}.csv\"\n",
    "    s3_file_name = f\"{FOLDER}/{key}\"\n",
    "    s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "\n",
    "    path_on_pc = 'E:/s3'\n",
    "    local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "    local_download_path = f'{path_on_pc}/{s3_file_name}'\n",
    "\n",
    "    if os.path.isfile(local_download_path) is False:\n",
    "    # создается новая папка, если ее нет\n",
    "        os.makedirs(local_download_folder, exist_ok=True)\n",
    "        s3.download_file(Bucket=BUCKET_NAME,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "    companies_st_partner_df = pd.read_csv(f'{local_download_path}')\n",
    "\n",
    "    # Объеденяем companies_st_partner и companies_dir_partner\n",
    "\n",
    "    companies_st_partner_df = companies_st_partner_df.merge(\n",
    "        companies_dir_partner_df,\n",
    "        on='partner_uuid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Объеденяем все данные\n",
    "\n",
    "    flats_st_partner_df1 = flats_st_partner_df.merge(\n",
    "        installation_point_st_partner_df[['digital_keys_count','device_keys_count','monetization','partner_uuid','monetization_is_allowed','installation_point_id','address_uuid','city','country','region','parent_uuid']],\n",
    "        on='address_uuid',\n",
    "        how='left'\n",
    "        )\n",
    "\n",
    "  \n",
    "    flats_st_partner_df_merged1 = flats_st_partner_df1.merge(\n",
    "        companies_st_partner_df[['pro_subs','is_blocked','enterprise_subs','billing_pro','balance','company_name','partner_lk','registration_date','tin','kpp','partner_uuid']],\n",
    "        on='partner_uuid',\n",
    "        how='left',\n",
    "        )\n",
    "\n",
    "    BUCKET_NAME = \"aggregated-data\"\n",
    "    FOLDER = 'flats_research_dashboard'\n",
    "\n",
    "    local_download_folder= f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[0,['date_key_folder']].values[0]}\"\n",
    "    local_download_path = f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[0,['date_key_parquet']].values[0]}\"\n",
    "\n",
    "    os.makedirs(local_download_folder, exist_ok=True)\n",
    "\n",
    "    flats_st_partner_df_merged1.to_parquet(local_download_path, compression='snappy', index=False)\n",
    "\n",
    "    s3_file_path = f\"{FOLDER}/{dates_pd.loc[0,['date_key_parquet']].values[0]}\"\n",
    "    pc_file_path = f\"{path_on_pc}/{FOLDER}/{dates_pd.loc[0,['date_key_parquet']].values[0]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.0.0-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\boris\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyarrow) (1.23.1)\n",
      "Downloading pyarrow-16.0.0-cp39-cp39-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 25.9/25.9 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-16.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650004 entries, 0 to 650003\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   report_date              650004 non-null  object \n",
      " 1   flat_uuid                650004 non-null  object \n",
      " 2   call_blocked             650004 non-null  int64  \n",
      " 3   blocked                  650004 non-null  int64  \n",
      " 4   deleted                  650004 non-null  int64  \n",
      " 5   address_uuid             649995 non-null  object \n",
      " 6   digital_keys_count       649945 non-null  float64\n",
      " 7   device_keys_count        649945 non-null  float64\n",
      " 8   monetization             649945 non-null  float64\n",
      " 9   partner_uuid             610769 non-null  object \n",
      " 10  monetization_is_allowed  649945 non-null  float64\n",
      " 11  installation_point_id    649945 non-null  float64\n",
      " 12  city                     649925 non-null  object \n",
      " 13  country                  649925 non-null  object \n",
      " 14  region                   648893 non-null  object \n",
      " 15  parent_uuid              649936 non-null  object \n",
      " 16  pro_subs                 610769 non-null  float64\n",
      " 17  is_blocked               610769 non-null  float64\n",
      " 18  enterprise_subs          610769 non-null  float64\n",
      " 19  billing_pro              610769 non-null  float64\n",
      " 20  balance                  610769 non-null  float64\n",
      " 21  company_name             608173 non-null  object \n",
      " 22  partner_lk               610769 non-null  float64\n",
      " 23  registration_date        610769 non-null  object \n",
      " 24  tin                      364400 non-null  object \n",
      " 25  kpp                      278078 non-null  object \n",
      "dtypes: float64(11), int64(3), object(12)\n",
      "memory usage: 128.9+ MB\n"
     ]
    }
   ],
   "source": [
    "flats_st_partner_df_merged1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650004 entries, 0 to 650003\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   report_date              650004 non-null  object \n",
      " 1   flat_uuid                650004 non-null  object \n",
      " 2   call_blocked             650004 non-null  int64  \n",
      " 3   blocked                  650004 non-null  int64  \n",
      " 4   deleted                  650004 non-null  int64  \n",
      " 5   address_uuid             649995 non-null  object \n",
      " 6   digital_keys_count       649945 non-null  float64\n",
      " 7   device_keys_count        649945 non-null  float64\n",
      " 8   monetization             649945 non-null  float64\n",
      " 9   partner_uuid             610769 non-null  object \n",
      " 10  monetization_is_allowed  649945 non-null  float64\n",
      " 11  installation_point_id    649945 non-null  float64\n",
      " 12  city                     649925 non-null  object \n",
      " 13  country                  649925 non-null  object \n",
      " 14  region                   648893 non-null  object \n",
      " 15  parent_uuid              649936 non-null  object \n",
      " 16  pro_subs                 610769 non-null  float64\n",
      " 17  is_blocked               610769 non-null  float64\n",
      " 18  pro_subs                 610769 non-null  float64\n",
      " 19  enterprise_subs          610769 non-null  float64\n",
      " 20  billing_pro              610769 non-null  float64\n",
      " 21  balance                  610769 non-null  float64\n",
      " 22  company_name             608173 non-null  object \n",
      " 23  partner_lk               610769 non-null  float64\n",
      " 24  registration_date        610769 non-null  object \n",
      " 25  tin                      364400 non-null  object \n",
      " 26  kpp                      278078 non-null  object \n",
      "dtypes: float64(12), int64(3), object(12)\n",
      "memory usage: 133.9+ MB\n"
     ]
    }
   ],
   "source": [
    "flats_st_partner_df_merged1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650004 entries, 0 to 650003\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   report_date              650004 non-null  object \n",
      " 1   flat_uuid                650004 non-null  object \n",
      " 2   call_blocked             650004 non-null  int64  \n",
      " 3   blocked                  650004 non-null  int64  \n",
      " 4   deleted                  650004 non-null  int64  \n",
      " 5   address_uuid             649995 non-null  object \n",
      " 6   digital_keys_count       649945 non-null  float64\n",
      " 7   device_keys_count        649945 non-null  float64\n",
      " 8   monetization             649945 non-null  float64\n",
      " 9   partner_uuid             610769 non-null  object \n",
      " 10  monetization_is_allowed  649945 non-null  float64\n",
      " 11  installation_point_id    649945 non-null  float64\n",
      " 12  city                     649925 non-null  object \n",
      " 13  country                  649925 non-null  object \n",
      " 14  region                   648893 non-null  object \n",
      " 15  parent_uuid              649936 non-null  object \n",
      " 16  pro_subs                 610769 non-null  float64\n",
      " 17  is_blocked               610769 non-null  float64\n",
      " 18  pro_subs                 610769 non-null  float64\n",
      " 19  enterprise_subs          610769 non-null  float64\n",
      " 20  billing_pro              610769 non-null  float64\n",
      " 21  balance                  610769 non-null  float64\n",
      " 22  company_name             608173 non-null  object \n",
      " 23  partner_lk               610769 non-null  float64\n",
      " 24  registration_date        610769 non-null  object \n",
      " 25  tin                      364400 non-null  object \n",
      " 26  kpp                      278078 non-null  object \n",
      "dtypes: float64(12), int64(3), object(12)\n",
      "memory usage: 133.9+ MB\n"
     ]
    }
   ],
   "source": [
    "flats_st_partner_df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    s3.upload_file(pc_file_path, BUCKET_NAME, s3_file_path)\n",
    "    print(f'{s3_file_path} the file has already been uploaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpython\u001b[49m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
