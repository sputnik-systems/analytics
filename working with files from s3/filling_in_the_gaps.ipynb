{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from config import ACCESS_KEY, SECRET_KEY, TOKEN\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "token = TOKEN\n",
    "headers={'Authorization':token,'Accept':'application/json'}\n",
    "\n",
    "#'b1gb310irjlk6b99e14g' - аналитика\n",
    "#'b1gc7vi2ckqausoc5dr7' - спутник\n",
    "\n",
    "FOLDER_ID = 'b1gc7vi2ckqausoc5dr7' # id каталога из которого береться запрос\n",
    "ACCESS_KEY = ACCESS_KEY #aws_access_key_id для S3\n",
    "SECRET_KEY = SECRET_KEY #aws_secret_access_key в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_instance():\n",
    "    session = boto3.session.Session()\n",
    "    return session.client(\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "\n",
    "# Если нужны все файлы, то не прописываем Prefix то Prefix=False\n",
    "def list_of_daily_objects_from_s3(bucket_name, Prefix):\n",
    "    s3 = get_s3_instance()\n",
    "    # Создаем пагинатор. Он нужен на тот случай, если файлов больше 1000\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "    # Присваеваем пагинатор с параметрами. \n",
    "    if folder is False:\n",
    "        page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "    else:    \n",
    "        page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=Prefix)\n",
    "\n",
    "    df_obj = pd.DataFrame({\n",
    "        'folder': [],\n",
    "        'year': [],\n",
    "        'month': [],\n",
    "        'day': [],\n",
    "        'key':[],\n",
    "        'LastModified':[],\n",
    "        }) # создаем пустой датасет для анализа\n",
    "\n",
    "    df_obj = df_obj.astype(str)\n",
    "\n",
    "    obj_number = 0 \n",
    "    for page in page_iterator:\n",
    "        for obj in page['Contents']: # list_objects - список объектов в бакете, Prefix - поиск во ключевому слову\n",
    "            if len(obj['Key'].split(sep='/')) == 4 \\\n",
    "            and obj['Key'].split(sep='/')[3]!='':   # условия фильтрации\n",
    "                df_obj.loc[obj_number,['folder']] = obj['Key'].split(sep='/')[0]\n",
    "                df_obj.loc[obj_number,['year']] = obj['Key'].split(sep='/')[1].replace('year=','')\n",
    "                df_obj.loc[obj_number,['month']] = obj['Key'].split(sep='/')[2].replace('month=','')\n",
    "                df_obj.loc[obj_number,['day']] = obj['Key'].split(sep='/')[3].replace('.csv','')\n",
    "                df_obj.loc[obj_number,['key']] = obj['Key']\n",
    "                df_obj.loc[obj_number,['LastModified']] = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S',)\n",
    "            obj_number += 1 \n",
    "                # добавляем в датасет найденные значения\n",
    "                \n",
    "    df_obj['date'] = df_obj['year']+'-'+df_obj['month']+'-'+df_obj['day']\n",
    "    df_obj['date'] = pd.to_datetime(df_obj['date'], dayfirst=False)\n",
    "\n",
    "    df_obj = df_obj.reset_index().drop('index', axis=1)\n",
    "    return df_obj\n",
    "\n",
    "\n",
    "def list_of_missing_data(df_obj):\n",
    "    s3 = get_s3_instance()\n",
    "    # задаем стартовую и финишную дату на основе ключей\n",
    "    start_date = df_obj.loc[0,['date']].values[0]\n",
    "    end_date = df_obj.loc[df_obj.shape[0]-1,['date']].values[0]\n",
    "\n",
    "    # создаем датасет с всеми датами\n",
    "    dates_pd = pd.DataFrame({\n",
    "        'date_range': pd.date_range(start=start_date, end=end_date),\n",
    "        })\n",
    "\n",
    "    # соединяем ранее полученный список и полный список дат\n",
    "    dates_merged = dates_pd.merge(\n",
    "        df_obj,\n",
    "        left_on='date_range',\n",
    "        right_on='date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # фильтруем полученный список по null строкам и выносим даты в отедльный датафрейм\n",
    "    df_of_missing_dates = pd.DataFrame()\n",
    "    df_of_missing_dates['date'] = dates_merged[dates_merged['key'].isnull()]['date_range'] \n",
    "    df_of_missing_dates = df_of_missing_dates.reset_index().drop('index', axis=1)\n",
    "\n",
    "    list_of_missing_dates = []\n",
    "\n",
    "    # Подставляем под пропущенные даты ключ файла, из которого будет браться информация\n",
    "    for i in range(df_of_missing_dates.shape[0]):\n",
    "\n",
    "        next_date_day = int((df_of_missing_dates.loc[i,['date']] + datetime.timedelta(days=1)).iloc[0].strftime('%d'))\n",
    "        next_date_month = (df_of_missing_dates.loc[i,['date']] + datetime.timedelta(days=1)).iloc[0].strftime('%m')\n",
    "        next_date_year = (df_of_missing_dates.loc[i,['date']] + datetime.timedelta(days=1)).iloc[0].strftime('%Y')\n",
    "        s3_file_name = f'{folder}/year={next_date_year}/month={next_date_month}/{next_date_day}.csv'\n",
    "        try: \n",
    "            if s3.head_object(Bucket=bucket_name,Key=s3_file_name)['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "                df_of_missing_dates.loc[i,['next file key']] = s3_file_name\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # заполняем оставшиеся пропуски следующим не пустым значением\n",
    "    # эта таблица  поможет выполнить дальнейшие действия\n",
    "    df_of_missing_dates = df_of_missing_dates.bfill()\n",
    "    return df_of_missing_dates\n",
    "\n",
    "\n",
    "# В качестве ключей для скачивания используем ключи всех файлов из папки, полученные ранее\n",
    "\n",
    "def downloading_missing_files_from_s3(path_on_pc,df_of_missing_dates):\n",
    "    s3 = get_s3_instance()\n",
    "    for i in range(0,df_of_missing_dates.shape[0]):\n",
    "        s3_file_name = df_of_missing_dates.loc[i,['next file key']].values[0]\n",
    "        s3_file_name_folder = '/'.join(s3_file_name.split(sep='/')[0:3])\n",
    "        \n",
    "        local_download_folder = f'{path_on_pc}/{s3_file_name_folder}'\n",
    "        local_download_path = f'{path_on_pc}/{s3_file_name}' \n",
    "        \n",
    "    # если данный файл уже скачан, он не скачивается повторно\n",
    "        if os.path.isfile(local_download_path) is False:\n",
    "        # создается новая папка, если ее нет\n",
    "            os.makedirs(local_download_folder, exist_ok=True)\n",
    "            s3.download_file(Bucket=bucket_name,Key=s3_file_name,Filename=local_download_path)\n",
    "\n",
    "\n",
    "def adding_missing_data_to_pc(folder, path_on_pc, df_of_missing_dates):\n",
    "    # проходимся по датафрейму пропущенных значений\n",
    "    for i in range(df_of_missing_dates.shape[0]):\n",
    "        new_date = df_of_missing_dates.loc[i,[\"date\"]].values[0].strftime('%Y-%m-%d')\n",
    "        df_of_missing_dates.loc[i,[\"date\"]].values[0].strftime('year=%Y/month=%m/%d.csv')\n",
    "        temp_df = pd.read_csv(f'{path_on_pc}/{df_of_missing_dates.loc[i,[\"next file key\"]].values[0]}')\n",
    "        # Обновляем дату\n",
    "        temp_df['report_date'] = new_date\n",
    "        temp_df.to_csv(f\"{path_on_pc}/{folder}/{df_of_missing_dates.loc[i,['date']].values[0].strftime('year=%Y/month=%m/%#d.csv')}\" ,sep=',',index=False)\n",
    "        # Создаем внутри датасета отдельный список ключей для загрузки\n",
    "        df_of_missing_dates.loc[i,['new_key']] = f\"{folder}/{df_of_missing_dates.loc[i,['date']].values[0].strftime('year=%Y/month=%m/%#d.csv')}\"\n",
    "\n",
    "\n",
    "def upploading_missing_data_to_storage(bucket_name,path_on_pc):\n",
    "    s3 = get_s3_instance()\n",
    "    for i in range(0,df_of_missing_dates.shape[0]):\n",
    "        s3_file_path = df_of_missing_dates.loc[i,['new_key']].values[0]\n",
    "        pc_file_path = f'{path_on_pc}/{s3_file_path}'\n",
    "        try:\n",
    "            try:\n",
    "                if s3.head_object(Bucket=bucket_name,Key=s3_file_path)['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "                    print(f'{s3_file_path} the file has already been uploaded!')\n",
    "            except:\n",
    "                s3.upload_file(pc_file_path, bucket_name, s3_file_path)\n",
    "                print(f'{s3_file_path} has been uploaded')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras_st_asgard/year=2024/month=08/2.csv has been uploaded\n",
      "cameras_st_asgard/year=2024/month=08/3.csv has been uploaded\n",
      "cameras_st_asgard/year=2024/month=08/4.csv has been uploaded\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'dwh-asgard' # имя бакета\n",
    "s3 = get_s3_instance()\n",
    "\n",
    "folder = 'cameras_st_asgard' # имя папки в бакете\n",
    "Prefix = f\"{folder}/\"\n",
    "df_obj = list_of_daily_objects_from_s3(bucket_name,Prefix)\n",
    "\n",
    "df_of_missing_dates =  list_of_missing_data(df_obj)\n",
    "\n",
    "path_on_pc = 'D:/s3' \n",
    "downloading_missing_files_from_s3(path_on_pc,df_of_missing_dates)\n",
    "adding_missing_data_to_pc(folder, path_on_pc, df_of_missing_dates)\n",
    "upploading_missing_data_to_storage(bucket_name,path_on_pc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
