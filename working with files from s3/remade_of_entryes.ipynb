{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть два неполных источника данных об монетизации. При чем один из них ранее выкладывался еще и с ошибками и требует замены.  Нужно как можно более полно и точно восстановать данные за предыдущие периуды, и подготовить код для functions, который будет восстанавливать данные на постоянку, пока новая выгрузка не запуститься. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from config import ACCESS_KEY, SECRET_KEY, TOKEN\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = TOKEN\n",
    "headers={'Authorization':token,'Accept':'application/json'}\n",
    "\n",
    "#'b1gb310irjlk6b99e14g' - аналитика\n",
    "#'b1gc7vi2ckqausoc5dr7' - спутник\n",
    "\n",
    "FOLDER_ID = 'b1gc7vi2ckqausoc5dr7' # id каталога из которого береться запрос\n",
    "ACCESS_KEY = ACCESS_KEY #aws_access_key_id для S3\n",
    "SECRET_KEY = SECRET_KEY #aws_secret_access_key в s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'dwh-asgard' # имя бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем подключение к yndex storage через .session.Session()\n",
    "\n",
    "def get_s3_instance():\n",
    "    session = boto3.session.Session()\n",
    "    return session.client(\n",
    "        aws_access_key_id=ACCESS_KEY,\n",
    "        aws_secret_access_key=SECRET_KEY,\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "\n",
    "s3 = get_s3_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_pc = 'C:/Users/Boris/Desktop/s3' \n",
    "date = '2024-03-22'\n",
    "path = datetime.datetime.strptime(date,'%Y-%m-%d').strftime('year=%Y/month=%m/%d.csv')\n",
    "\n",
    "entries_df = pd.read_csv(f'{path_on_pc}/entries/{path}')\n",
    "entries_st_mobile_df = pd.read_csv(f'{path_on_pc}/entries_st_mobile/{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_df['monetization'] = entries_df['subsc_restrict'].apply(lambda x: \"True\" if x == 'freemonetization' else \"False\")\n",
    "entries_df['r_address_uuid'] = entries_df['address_uuid']\n",
    "\n",
    "entries_st_mobile_df = pd.read_csv(f'{path_on_pc}/entries_st_mobile/{path}')\n",
    "entries_st_mobile_df['l_address_uuid'] = entries_st_mobile_df['address_uuid']\n",
    "\n",
    "merget_df = entries_st_mobile_df.merge(\n",
    "        entries_df[['r_address_uuid','address','monetization','partner_uuid']],\n",
    "        left_on='l_address_uuid', \n",
    "        right_on='r_address_uuid',\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Находим подъезды без домофонов из entries_st_mobile, которых нет в entries\n",
    "#Удоляем дубликаты и переменовываем\n",
    "\n",
    "merget_df_unique_from_entries_st_mobile = merget_df[merget_df['r_address_uuid'] \\\n",
    "    .isnull()][['report_date','address_uuid','partner_uuid_left','monetization_left']]\n",
    "merget_df_unique_from_entries_st_mobile = merget_df_unique_from_entries_st_mobile \\\n",
    "    .rename(columns={'partner_uuid_left':'partner_uuid','partner_uuid_left':'partner_uuid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#оставляем для повтороного соединения только определенные строки без дубликатов из исходника \n",
    "\n",
    "entries_df_for_concatenation = entries_df[['report_date','address_uuid','partner_uuid','monetization']]\n",
    "entries_df_for_concatenation = entries_df_for_concatenation.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# соединяем и проверяем на дубликаты\n",
    "df_concated = pd.concat([entries_df_for_concatenation, merget_df_unique_from_entries_st_mobile]).reset_index().drop('index', axis=1)\n",
    "df_concated['address_uuid'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем вспосогательнаый датафрейм\n",
    "\n",
    "start_date = datetime.datetime.strptime('2023-04-28','%Y-%m-%d').date()\n",
    "end_date = datetime.datetime.strptime('2024-02-22','%Y-%m-%d').date()\n",
    "dates_pd = pd.DataFrame({\n",
    "        'date_range': pd.date_range(start=start_date, end=end_date),\n",
    "        'date_key': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m/%d.csv'),\n",
    "        'date_key_folder': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m')\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_on_pc = 'C:/Users/Boris/Desktop/s3' \n",
    "\n",
    "\n",
    "for i in range(0,dates_pd.shape[0]):\n",
    "    key = dates_pd.loc[i,['date_key']].values[0]\n",
    "    key_folder = dates_pd.loc[i,['date_key_folder']].values[0]\n",
    "\n",
    "    # переделываем данные из entryes для всех предыдущих дат и добавляем туда наиболее близкую по дате инфу из entryes_st_mobile\n",
    "    if os.path.isfile(f'{path_on_pc}/new_entryes_st_mobile/{key}') is False:\n",
    "        entries_df = pd.read_csv(f'{path_on_pc}/entries/{key}')\n",
    "        entries_df['monetization'] = entries_df['subsc_restrict'].apply(lambda x: \"True\" if x == 'freemonetization' else \"False\")\n",
    "        entries_df_for_concatenation = entries_df[['report_date','address_uuid','partner_uuid','monetization']]\n",
    "        entries_df_for_concatenation = entries_df_for_concatenation.drop_duplicates()\n",
    "        df_concated = pd.concat([entries_df_for_concatenation, merget_df_unique_from_entries_st_mobile]).reset_index().drop('index', axis=1)\n",
    "        os.makedirs(f'{path_on_pc}/new_entryes_st_mobile/{key_folder}', exist_ok=True)\n",
    "        df_concated.to_csv(f'{path_on_pc}/new_entryes_st_mobile/{key}',sep=',', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем новый вспомогетельный датасет для обновления соединения относительно верных данных\n",
    "\n",
    "start_date = datetime.datetime.strptime('2024-04-10','%Y-%m-%d').date()\n",
    "end_date = datetime.datetime.strptime('2024-04-16','%Y-%m-%d').date()\n",
    "dates_pd = pd.DataFrame({\n",
    "        'date_range': pd.date_range(start=start_date, end=end_date),\n",
    "        'date_key': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m/%d.csv'),\n",
    "        'date_key_folder': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m')\n",
    "        })\n",
    "\n",
    "\n",
    "path_on_pc = 'C:/Users/Boris/Desktop/s3' \n",
    "\n",
    "    # переделываем данные из entryes после 2024-02-23 и добавляем туда наиболее близкую по дате инфу из entryes_st_mobile\n",
    "\n",
    "for i in range(0,dates_pd.shape[0]):\n",
    "    key = dates_pd.loc[i,['date_key']].values[0]\n",
    "    key_folder = dates_pd.loc[i,['date_key_folder']].values[0]\n",
    "    if os.path.isfile(f'{path_on_pc}/new_entryes_st_mobile/{key}') is False:\n",
    "        entries_df = pd.read_csv(f'{path_on_pc}/entries/{key}')\n",
    "        entries_df['monetization'] = entries_df['subsc_restrict'].apply(lambda x: \"True\" if x == 'freemonetization' else \"False\")\n",
    "        entries_df['r_address_uuid'] = entries_df['address_uuid']\n",
    "        \n",
    "        entries_st_mobile_df = pd.read_csv(f'{path_on_pc}/entries_st_mobile/{path}')\n",
    "        entries_st_mobile_df['l_address_uuid'] = entries_st_mobile_df['address_uuid']\n",
    "\n",
    "        merget_df = entries_st_mobile_df.merge(\n",
    "            entries_df[['r_address_uuid','address','monetization','partner_uuid']],\n",
    "            left_on='l_address_uuid', \n",
    "            right_on='r_address_uuid',\n",
    "            how='outer',\n",
    "            suffixes=('_left', '_right'))\n",
    "        \n",
    "        \n",
    "        merget_df_unique_from_entries_st_mobile = merget_df[merget_df['r_address_uuid'] \\\n",
    "            .isnull()][['report_date','address_uuid','partner_uuid_left','monetization_left']]\n",
    "        merget_df_unique_from_entries_st_mobile = merget_df_unique_from_entries_st_mobile \\\n",
    "            .rename(columns={'partner_uuid_left':'partner_uuid','partner_uuid_left':'partner_uuid'})\n",
    "        \n",
    "        entries_df_for_concatenation = entries_df[['report_date','address_uuid','partner_uuid','monetization']]\n",
    "        entries_df_for_concatenation = entries_df_for_concatenation.drop_duplicates()\n",
    "        df_concated = pd.concat([entries_df_for_concatenation, merget_df_unique_from_entries_st_mobile]).reset_index().drop('index', axis=1)\n",
    "        os.makedirs(f'{path_on_pc}/new_entryes_st_mobile/{key_folder}', exist_ok=True)\n",
    "        df_concated.to_csv(f'{path_on_pc}/new_entryes_st_mobile/{key}',sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем на дубликаты\n",
    "df_concated['address_uuid'].value_counts().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем вспомогательный датасет для загрузки данных\n",
    "\n",
    "start_date = datetime.datetime.strptime('2024-04-10','%Y-%m-%d').date()\n",
    "end_date = datetime.datetime.strptime('2024-04-16','%Y-%m-%d').date()\n",
    "dates_pd = pd.DataFrame({\n",
    "        'date_range': pd.date_range(start=start_date, end=end_date),\n",
    "        'date_key': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m/%d.csv'),\n",
    "        'date_key_folder': pd.date_range(start=start_date, end=end_date).strftime('year=%Y/month=%m')\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year=2023/month=06/14.csv'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_pd.loc[i,['date_key']].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2024/month=04/10.csv has been uploaded\n",
      "year=2024/month=04/11.csv has been uploaded\n",
      "year=2024/month=04/12.csv has been uploaded\n",
      "year=2024/month=04/13.csv has been uploaded\n",
      "year=2024/month=04/14.csv has been uploaded\n",
      "year=2024/month=04/15.csv has been uploaded\n",
      "year=2024/month=04/16.csv has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# загружаем данные\n",
    "\n",
    "for i in range(0,dates_pd.shape[0]):\n",
    "    key = dates_pd.loc[i,['date_key']].values[0]\n",
    "    s3_file_path = f'entries_st_mobile/{key}'\n",
    "    pc_file_path = f'{path_on_pc}/new_entryes_st_mobile/{key}'\n",
    "    try:\n",
    "        s3.upload_file(pc_file_path, bucket_name, s3_file_path)\n",
    "        print(f'{key} has been uploaded')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Упс, ошибка в имени папки. Копирую внутри бакета в ту папку, в которую нужно\n",
    "\n",
    "for i in range(0,dates_pd.shape[0]):\n",
    "    key = dates_pd.loc[i,['date_key']].values[0]\n",
    "    s3_file_path_old = f'entryes_st_mobile/{key}'\n",
    "    s3_file_path_new = f'entries_st_mobile/{key}'\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket_name,\n",
    "        CopySource=f'{bucket_name}/{s3_file_path_old}',\n",
    "        Key=s3_file_path_new\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
